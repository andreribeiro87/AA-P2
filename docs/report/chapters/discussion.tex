\section{Discussion}
\label{sec:discussion}

This section analyzes the experimental findings, comparing theoretical
predictions with practical observations, and identifying trade-offs and failure
modes of each algorithm category.

\subsection{Theoretical vs. Practical Complexity}
\label{sec:theory-vs-practice}

\subsubsection{Randomized Algorithms}

Theoretical $O(Tn^2)$ complexity matches observations, execution time grows
quadratically with graph size and linearly with the iteration count. However,
practical performance varies significantly:

\begin{itemize}
    \item \textbf{Random Construction} performs better than expected because clique construction terminates early when no compatible vertices remain, reducing the effective complexity.
    \item \textbf{Monte Carlo} exhibits similar scaling but with higher constant factors due to probability computations.
    \item \textbf{Las Vegas} shows variable runtime as expected, with worst-case behavior on dense graphs where many random attempts fail to improve.
    \item \textbf{Iterative Random Search} suffers from the low probability of randomly generating large cliques, explaining its poor quality results.
\end{itemize}

\subsubsection{Exact Algorithms}

Theoretically exponential, these algorithms show much better practical
behavior.

\begin{itemize}
    \item \textbf{WLMC and TSM-MWC} benefit enormously from preprocessing, often reducing graph size by 50-80\% before search begins.
    \item Upper bound pruning eliminates most of the search space, making the algorithms
          practical for graphs up to $n=100$.
    \item Density has a pronounced effect: sparse graphs (12.5\%) are solved orders of
          magnitude faster than dense graphs (75\%).
\end{itemize}

\subsubsection{Reduction-Based Algorithms}

\begin{itemize}
    \item \textbf{MWCRedu's} $O(n^3)$ preprocessing dominates for small graphs but pays off for larger instances.
    \item The effectiveness of reduction rules depends heavily on graph
          structure---random graphs with uniform weights offer fewer reduction
          opportunities.
\end{itemize}

\subsection{Accuracy vs. Speed Trade-offs}
\label{sec:tradeoffs}

The results reveal distinct trade-off profiles:

\begin{table}[H]
    \centering
    \caption{Algorithm Trade-off Summary}\label{tab:tradeoffs}
    \scalebox{0.85}{

        \begin{tabular}{lccc}
            \toprule
            \textbf{Algorithm}   & \textbf{Speed} & \textbf{Quality} & \textbf{Recommended Use}    \\
            \midrule
            random\_construction & Fast           & High             & General purpose             \\
            monte\_carlo         & Medium         & High             & Probabilistic bounds        \\
            las\_vegas           & Variable       & Medium           & Correctness critical        \\
            mwc\_redu            & Very Fast      & Medium           & Large graphs, quick answer  \\
            max\_clique\_weight  & Slow           & Optimal          & Medium graphs, exact needed \\
            wlmc                 & Medium         & Optimal          & Large sparse graphs         \\
            fast\_wclq           & Fast           & Very High        & Production systems          \\
            scc\_walk            & Slow           & High             & Local refinement            \\
            \bottomrule
        \end{tabular}
    }
\end{table}

\subsection{Best Use Cases}
\label{sec:use-cases}

Based on the analysis, the following recommendations are made:

\subsubsection{For Optimal Solutions}
\begin{itemize}
    \item \textbf{Small graphs ($n < 20$):} Any exact algorithm works; \texttt{exhaustive} for simplicity.
    \item \textbf{Medium graphs ($20 \leq n < 50$):} \texttt{wlmc} or \texttt{tsm\_mwc} with reasonable time limits.
    \item \textbf{Large sparse graphs:} \texttt{wlmc} with preprocessing excels.
    \item \textbf{Large dense graphs:} Consider \texttt{max\_clique\_dyn\_weight} for tighter bounds.
\end{itemize}

\subsubsection{For Fast Approximate Solutions}
\begin{itemize}
    \item \textbf{General use:} \texttt{random\_construction} offers surprising quality with speed.
    \item \textbf{Quality-critical:} \texttt{fast\_wclq} balances speed with near-optimal results.
    \item \textbf{Iterative refinement:} Start with \texttt{greedy}, refine with \texttt{scc\_walk}.
\end{itemize}

\subsubsection{For Real-time Applications}
\begin{itemize}
    \item \texttt{mwc\_redu} with greedy solver provides fastest results.
    \item Single-start greedy for microsecond latency requirements.
\end{itemize}

\subsection{Experimental vs. Formal Complexity Comparison}
\label{sec:exp-vs-formal}

A key objective of this study is comparing theoretical complexity predictions
with empirical measurements.

\subsubsection{Validation Methodology}

For each algorithm, the following steps were performed:
\begin{itemize}
    \item The theoretical complexity formula was fitted to experimental data using
          least-squares regression.
    \item The correlation coefficient ($R^2$) between predicted and measured execution
          times was computed.
    \item Deviations were analyzed to identify constants hidden in $O(\cdot)$ notation.
\end{itemize}

\subsubsection{Validation Results}

\begin{table}[h!]
    \centering
    \caption{Experimental vs. Theoretical Complexity Validation}\label{tab:exp-vs-formal}
    \scalebox{0.8}{

        \begin{tabular}{lccc}
            \toprule
            \textbf{Algorithm}   & \textbf{Theoretical} & \textbf{Fitted Model}                            & $R^2$ \\
            \midrule
            random\_construction & $O(Tn^2)$            & $T(n) = 7.68 \times 10^{-5} n^2$                 & 0.90  \\
            monte\_carlo         & $O(Tn^2)$            & $T(n) = 1.80 \times 10^{-4} n^2$                 & 0.87  \\
            las\_vegas           & $O(Tn^2)$            & $T(n) = 7.86 \times 10^{-5} n^2$                 & 0.83  \\
            greedy               & $O(n^2)$             & $T(n) = 1.38 \times 10^{-6} n^2$                 & 0.97  \\
            exhaustive           & $O(2^n n)$           & $T(n) = 5.05 \times 10^{-8} \cdot 2^n n$         & 0.97  \\
            wlmc                 & $O(2^k n^2)$         & $T(n) = 1.40 \times 10^{-6} \cdot 2^{0.02n} n^2$ & 0.96  \\
            \bottomrule
        \end{tabular}
    }
\end{table}

\textbf{Key observations:}
\begin{itemize}
    \item \textbf{Quadratic algorithms} ($O(n^2)$) show excellent correlation ($R^2 > 0.95$), confirming theoretical predictions.
    \item \textbf{Exponential algorithms} show more variance due to instance-specific pruning efficiency.
    \item \textbf{Hidden constants} vary by 50$\times$ between algorithms with same asymptotic complexity (e.g., random\_construction vs. greedy).
    \item \textbf{Branch-and-bound} correlation is lower due to data-dependent execution paths.
\end{itemize}

The complete complexity validation for all 15 algorithms is provided in
Appendix~\ref{sec:appendix-complexity}.

\subsubsection{Deviation Analysis, between theory and practice}

\begin{enumerate}
    \item \textbf{Early termination:} WLMC and TSM-MWC often terminate before exploring worst-case configurations, performing 10-100$\times$ faster than worst-case bounds suggest.

    \item \textbf{Cache effects:} Algorithms with better memory locality (e.g., greedy with sorted vertex list) outperform theoretical predictions for small $n$.

    \item \textbf{Graph structure:} Random graphs with uniform density behave differently from real-world graphs with community structure.

    \item \textbf{Python overhead:} Interpreted execution adds constant overhead that dominates for small $n$, making asymptotic analysis visible only for $n > 50$.
\end{enumerate}

\subsubsection{Practical Implications}

Correlation of $R^2 > 0.95$ for polynomial algorithms confirms that $O(\cdot)$
analysis accurately predicts relative performance scaling. However, absolute
time prediction requires empirical constant estimation:

\begin{equation}
    T_{actual}(n) = c \cdot T_{theoretical}(n)
\end{equation}

where $c$ must be determined experimentally for each algorithm-platform
combination.

\subsection{Failure Points and Limitations}
\label{sec:failures}

\subsubsection{Algorithm-Specific Failures}

\begin{itemize}
    \item \textbf{Iterative Random Search:} Fundamentally flawed for MWC---the probability of randomly sampling a maximum clique decreases exponentially with clique size. The implementation found essentially no valid solutions.

    \item \textbf{MWCPeel:} Aggressive peeling removes vertices that may be crucial for optimal cliques. Average quality of 79\% with some instances as low as 5.56\% indicates severe failure modes.

    \item \textbf{Las Vegas:} While guaranteeing correctness, it can get stuck in local optima. The 50.30\% minimum quality suggests some graph structures are pathological for random walk strategies.

    \item \textbf{MWCRedu:} Graph reduction effectiveness varies. On random graphs with uniform structure, fewer vertices are dominated, limiting reduction benefits.
\end{itemize}

\subsubsection{Density-Related Failures}

Higher density consistently degrades performance across all algorithms:
\begin{itemize}
    \item \textbf{Exact algorithms:} Exponentially more configurations to explore.
    \item \textbf{Randomized:} More compatible vertices at each step increases construction cost.
    \item \textbf{Local search:} Larger neighborhoods slow down move evaluation.
\end{itemize}

\subsubsection{Size-Related Failures}

\begin{itemize}
    \item \textbf{Exhaustive:} Becomes impractical beyond $n \approx 22$ due to $2^n$ configurations.
    \item \textbf{Branch-and-bound:} Without time limits, can still take exponential time on adversarial instances.
\end{itemize}

\subsection{Surprising Results}
\label{sec:surprises}

Several results were unexpected:

\begin{enumerate}
    \item \textbf{Random Construction achieving 100\% optimality} on test cases suggests that simple random construction with multiple restarts is highly effective for MWC, likely because the greedy extension phase tends to find maximal cliques of competitive weight.

    \item \textbf{SCCWalk underperforming FastWClq} despite being a more sophisticated local search. The BMS strategy in FastWClq appears more effective than SCC's cycling avoidance.

\end{enumerate}

\subsection{Recommendations for Practitioners}
\label{sec:recommendations}

\begin{enumerate}
    \item \textbf{Default choice:} Use \texttt{fast\_wclq} for most applications---it combines speed with reliability.

    \item \textbf{When optimality matters:} Use \texttt{wlmc} with appropriate time limits.

    \item \textbf{For very large graphs:} Apply \texttt{mwc\_redu} preprocessing, then use \texttt{fast\_wclq} on the reduced graph.

    \item \textbf{Avoid:} \texttt{iterative\_random\_search} (poor quality) and \texttt{mwc\_peel} (unpredictable quality loss).

    \item \textbf{Benchmark first:} Algorithm performance varies with graph structure. Test on representative instances before deploying.
\end{enumerate}

